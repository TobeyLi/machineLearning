{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the dataset\n",
    "---\n",
    "在load_data的时候，好像是不能指定我们读取的文件的路径，只能在将下载好的文件放在.keras/dataset/下，这样才不会出错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb=keras.datasets.imdb\n",
    "(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 先看看train_data 是长的什么样子,为一个标量\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing entries:25000,labels:25000\n"
     ]
    }
   ],
   "source": [
    "# 查看训练集的内容，查看训练数据以及标签\n",
    "print(\"Traing entries:{},labels:{}\".format(len(train_data),len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 68)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看语句的长度，看长度是否是一致的,如果不一致，那么后期就是需要进行处理，将他们padding成同等长度的句子\n",
    "len(train_data[0]),len(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在上面的内容，我们查看可word2index的内容，但是我们也可以查看原文的内容，这需要我们将要进行index2word操作\n",
    "word2index=imdb.get_word_index()\n",
    "\n",
    "# 获取到了word2index的内容，那么我们需要在这个字典的头部插入4个内容，然后将相关key对应的value值向后移\n",
    "word2index={k:(v+3) for k,v in word2index.items()}\n",
    "word2index[\"<PAD>\"]=0\n",
    "word2index[\"<START>\"]=1\n",
    "word2index[\"<UNK>\"]=2\n",
    "word2index[\"<UNUSED>\"]=3\n",
    "len(word2index)  # 字典的长度\n",
    "\n",
    "# 将word2index 逆转，使其变成index2word即{123：“hello”}的形式\n",
    "index2word=dict([(value,key) for (key,value) in word2index.items()])\n",
    "\n",
    "# 将一个列表转换为文字\n",
    "def decode_review(text):\n",
    "    # Python 字典 dict.get(key, default=None)函数返回指定键的值，如果值不在字典中返回默认值。\n",
    "    return \" \".join([index2word.get(i,'?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做到上面这一步，我们可以是对这个数据集有了大致的认识了，了解了这个数据集是怎么使用的，后面的内容主要是为了情感分析做准备\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分类\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length in trainset: 2494\n",
      "max length in testset: 2315\n"
     ]
    }
   ],
   "source": [
    "# 在上面的内容中，我们看到，每个句子的长度都是不相等的，这在文本处理中是行不通的，我们需要将这些句子padding成同等长度的句子，\n",
    "# 在这之前，为了不丢失信息，我们可以先查看train_data,test_data句子中最长的句子长度是多少\n",
    "def get_length(datas):\n",
    "    max_length=0\n",
    "    for data in datas:\n",
    "        if len(data)>max_length:\n",
    "            max_length=len(data)\n",
    "    return max_length\n",
    "print(\"max length in trainset:\",get_length(train_data))\n",
    "print(\"max length in testset:\",get_length(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于文本的处理来说，文本的长度尽量不要太长，因为后面如果采用RNNs的话，可能会遇到梯度消失和梯度爆炸的问题，所以我们需要进行一定的截断\n",
    "train_data=keras.preprocessing.sequence.pad_sequences(train_data,value=word2index[\"<PAD>\"],padding='post',maxlen=256)\n",
    "test_data=keras.preprocessing.sequence.pad_sequences(test_data,value=word2index[\"<PAD>\"],padding='post',maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "# 现在来查看长度,查看那边的长度是否都是已经变成了256\n",
    "print(get_length(train_data))\n",
    "print(get_length(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型\n",
    "---\n",
    "神经网络通过堆叠层创建而成，这需要做出两个架构方面的主要决策：\n",
    "\n",
    "1）要在模型中使用多少个层？  \n",
    "2）要针对每个层使用多少个隐藏单元？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0620 21:25:37.776062  1020 deprecation.py:506] From J:\\python\\env\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0620 21:25:37.835028  1020 deprecation.py:506] From J:\\python\\env\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          240000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 240,289\n",
      "Trainable params: 240,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size=15000\n",
    "# 序列化建图\n",
    "model=keras.Sequential()\n",
    "# embedding层，初始化内容为（vocab_size,embedding_dim）,下面是每个词都会被编辑为16维的向量,\n",
    "# 之后的维度变为(batch_size, sequence_len, embedding_dim)\n",
    "model.add(keras.layers.Embedding(vocab_size,16))\n",
    "# 平均池化\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "# 全连接层,该长度固定的输出向量会传入一个全连接 (Dense) 层（包含 16 个隐藏单元）。\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "# dropout层\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "# 应用 sigmoid 激活函数后，结果是介于 0 到 1 之间的浮点值，表示概率或置信水平。\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数与优化器\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0620 21:30:27.418706  1020 deprecation.py:323] From J:\\python\\env\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建验证集\n",
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 1s 100us/sample - loss: 0.6923 - acc: 0.5118 - val_loss: 0.6912 - val_acc: 0.5147\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.6884 - acc: 0.5903 - val_loss: 0.6858 - val_acc: 0.6376\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.6799 - acc: 0.6401 - val_loss: 0.6749 - val_acc: 0.6990\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.6646 - acc: 0.6818 - val_loss: 0.6566 - val_acc: 0.7592\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.6409 - acc: 0.7235 - val_loss: 0.6302 - val_acc: 0.7786\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.6101 - acc: 0.7629 - val_loss: 0.5974 - val_acc: 0.7888\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.5735 - acc: 0.7816 - val_loss: 0.5590 - val_acc: 0.8214\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.5329 - acc: 0.8067 - val_loss: 0.5205 - val_acc: 0.8340\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.4953 - acc: 0.8261 - val_loss: 0.4837 - val_acc: 0.8473\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.4626 - acc: 0.8384 - val_loss: 0.4518 - val_acc: 0.8535\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.4302 - acc: 0.8531 - val_loss: 0.4239 - val_acc: 0.8620\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.4049 - acc: 0.8603 - val_loss: 0.4022 - val_acc: 0.8614\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.3808 - acc: 0.8713 - val_loss: 0.3805 - val_acc: 0.8709\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.3600 - acc: 0.8809 - val_loss: 0.3657 - val_acc: 0.8710\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.3386 - acc: 0.8865 - val_loss: 0.3502 - val_acc: 0.8777\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.3217 - acc: 0.8955 - val_loss: 0.3394 - val_acc: 0.8753\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.3076 - acc: 0.8988 - val_loss: 0.3285 - val_acc: 0.8801\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2941 - acc: 0.9027 - val_loss: 0.3204 - val_acc: 0.8796\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2841 - acc: 0.9093 - val_loss: 0.3125 - val_acc: 0.8821\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2720 - acc: 0.9163 - val_loss: 0.3072 - val_acc: 0.8835\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.2604 - acc: 0.9187 - val_loss: 0.3029 - val_acc: 0.8829\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.2475 - acc: 0.9228 - val_loss: 0.2967 - val_acc: 0.8860\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.2363 - acc: 0.9297 - val_loss: 0.2941 - val_acc: 0.8846\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.2313 - acc: 0.9303 - val_loss: 0.2931 - val_acc: 0.8857\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2231 - acc: 0.9341 - val_loss: 0.2873 - val_acc: 0.8872\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.2135 - acc: 0.9369 - val_loss: 0.2874 - val_acc: 0.8862\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.2047 - acc: 0.9431 - val_loss: 0.2845 - val_acc: 0.8871\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.1961 - acc: 0.9446 - val_loss: 0.2855 - val_acc: 0.8876\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1900 - acc: 0.9469 - val_loss: 0.2826 - val_acc: 0.8887\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1839 - acc: 0.9495 - val_loss: 0.2827 - val_acc: 0.8891\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1774 - acc: 0.9522 - val_loss: 0.2821 - val_acc: 0.8882\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1713 - acc: 0.9528 - val_loss: 0.2837 - val_acc: 0.8882\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.1654 - acc: 0.9555 - val_loss: 0.2845 - val_acc: 0.8884\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.1625 - acc: 0.9565 - val_loss: 0.2854 - val_acc: 0.8887\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1556 - acc: 0.9602 - val_loss: 0.2857 - val_acc: 0.8881\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.1478 - acc: 0.9630 - val_loss: 0.2870 - val_acc: 0.8875\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1439 - acc: 0.9644 - val_loss: 0.2896 - val_acc: 0.8879\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.1418 - acc: 0.9655 - val_loss: 0.2920 - val_acc: 0.8869\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1349 - acc: 0.9685 - val_loss: 0.2966 - val_acc: 0.8865\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.1321 - acc: 0.9703 - val_loss: 0.2971 - val_acc: 0.8854\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估模型\n",
    "--\n",
    "我们来看看模型的表现如何。模型会返回两个值：损失（表示误差的数字，越低越好）和准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 39us/sample - loss: 0.3216 - acc: 0.8744\n",
      "[0.3215745053386688, 0.87444]\n"
     ]
    }
   ],
   "source": [
    "results=model.evaluate(test_data,test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建准确率和损失随时间变化的图\n",
    "---\n",
    "`model.fit()` 返回一个 History 对象，该对象包含一个字典，其中包括训练期间发生的所有情况：这个字典是如下形式的字典：  \n",
    "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict=history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字典中的元素取出来\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc=history_dict[\"acc\"]\n",
    "loss=history_dict[\"loss\"]\n",
    "val_loss=history_dict[\"val_loss\"]\n",
    "val_acc=history_dict[\"val_acc\"]\n",
    "\n",
    "# 确定画的点数【横坐标】\n",
    "epoch=range(1,len(acc)+1)\n",
    "# 画出训练集的loss\n",
    "plt.plot(epoch,loss,'bo',label=\"Training loss\")\n",
    "# 画出验证集的loss\n",
    "plt.plot(epoch,val_loss,'r',label=\"Validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7127f52b591a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# clear figure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epoch, acc, 'bo', label='Training acc')\n",
    "plt.plot(epoch, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
