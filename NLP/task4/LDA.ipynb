{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据，分词\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('京州', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一个文档分词#\n",
    "with open('../data/nlp_test0.txt','r',encoding='utf-8') as f:\n",
    "    document = f.read()\n",
    "    document_cut = jieba.cut(document)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('../data/nlp_test1.txt', 'w') as f2:\n",
    "        f2.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二个文档分词#\n",
    "with open('../data/nlp_test2.txt',encoding='utf-8') as f:\n",
    "    document2 = f.read()\n",
    "    document2_cut = jieba.cut(document2)\n",
    "    result = ' '.join(document2_cut)\n",
    "    with open('../data/nlp_test3.txt', 'w') as f2:\n",
    "        f2.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第三个文档分词#\n",
    "jieba.suggest_freq('桓温', True)\n",
    "with open('../data/nlp_test4.txt',encoding='utf-8') as f:\n",
    "    document3 = f.read()\n",
    "    document3_cut = jieba.cut(document3)\n",
    "    result = ' '.join(document3_cut)\n",
    "    with open('../data/nlp_test5.txt', 'w') as f3:\n",
    "        f3.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。\n",
      "沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。\n",
      "347 年 （ 永和 三年 ） 三月 ， 桓温 兵至 彭模 （ 今 四川 彭山 东南 ） ， 留下 参军 周楚 、 孙盛 看守 辎重 ， 自己 亲率 步兵 直攻 成都 。 同月 ， 成汉 将领 李福 袭击 彭模 ， 结果 被 孙盛 等 人 击退 ； 而 桓温 三 战三胜 ， 一直 逼近 成都 。\n"
     ]
    }
   ],
   "source": [
    "with open('../data/nlp_test1.txt') as f3:\n",
    "    res1 = f3.read()\n",
    "print(res1)\n",
    "with open('../data/nlp_test3.txt') as f4:\n",
    "    res2 = f4.read()\n",
    "print(res2)\n",
    "with open('../data/nlp_test5.txt') as f5:\n",
    "    res3 = f5.read()\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件导入停用词表\n",
    "stpwrdpath = \"../data/stop_words_uft8.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'r',encoding='utf-8')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 38)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 84)\t1\n",
      "  (0, 61)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 39)\t2\n",
      "  (0, 72)\t2\n",
      "  (0, 75)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 63)\t5\n",
      "  (0, 25)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 83)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 23)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 32)\t1\n",
      "  :\t:\n",
      "  (2, 71)\t1\n",
      "  (2, 53)\t1\n",
      "  (2, 40)\t1\n",
      "  (2, 45)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 46)\t2\n",
      "  (2, 66)\t1\n",
      "  (2, 57)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 78)\t1\n",
      "  (2, 68)\t1\n",
      "  (2, 36)\t2\n",
      "  (2, 29)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 64)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 42)\t1\n",
      "  (2, 31)\t1\n",
      "  (2, 43)\t2\n",
      "  (2, 19)\t1\n",
      "  (2, 55)\t2\n",
      "  (2, 7)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 59)\t1\n",
      "  (2, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# 接着我们要把词转化为词频向量，注意由于LDA是基于词频统计的，因此一般不用TF-IDF来做文档特征。\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# 合并文档\n",
    "corpus = [res1,res2,res3]\n",
    "cntVector = CountVectorizer(stop_words=stpwrdlst)\n",
    "cntTf = cntVector.fit_transform(corpus)\n",
    "print(cntTf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=2,\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "docres = lda.fit_transform(cntTf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98917922 0.01082078]\n",
      " [0.0168806  0.9831194 ]\n",
      " [0.98429349 0.01570651]]\n",
      "============================================================\n",
      "[[1.49883571 1.49883571 2.4991068  2.4991068  2.4991068  0.50044945\n",
      "  1.49883571 1.49883571 2.4991068  0.50044945 1.49883571 1.49913028\n",
      "  0.50044945 0.50044945 1.49913028 0.50046155 1.49883571 1.49913028\n",
      "  0.50044945 1.49883571 1.49883571 1.49913028 0.50046155 1.49913028\n",
      "  1.49913028 1.49913028 1.49883571 1.49883571 0.50044945 1.49883571\n",
      "  1.49913028 1.49883571 1.49913028 1.49913028 0.50044945 0.50044945\n",
      "  2.49880421 0.50044945 1.49913028 2.4991068  1.49883571 0.50044945\n",
      "  1.49883571 2.49880421 1.49913028 1.49883571 2.49880421 1.49883571\n",
      "  0.50046155 0.50044945 1.49913028 4.50618206 1.49913028 1.49883571\n",
      "  2.49986092 2.49880421 0.50046155 1.49883571 0.50046409 1.49883571\n",
      "  2.49986092 1.49913028 0.50044945 5.49719844 1.49883571 1.49913028\n",
      "  1.49883571 1.49913028 1.49883571 0.50044945 1.49913028 1.49883571\n",
      "  2.49986092 1.49913028 1.49913028 1.49913028 1.49913028 0.50044945\n",
      "  1.49883571 1.49913028 1.49883571 1.49913028 1.49913028 1.49913028\n",
      "  1.49913028]\n",
      " [0.50116429 0.50116429 0.5008932  0.5008932  0.5008932  1.49955055\n",
      "  0.50116429 0.50116429 0.5008932  1.49955055 0.50116429 0.50086972\n",
      "  1.49955055 1.49955055 0.50086972 2.49953845 0.50116429 0.50086972\n",
      "  1.49955055 0.50116429 0.50116429 0.50086972 2.49953845 0.50086972\n",
      "  0.50086972 0.50086972 0.50116429 0.50116429 1.49955055 0.50116429\n",
      "  0.50086972 0.50116429 0.50086972 0.50086972 1.49955055 1.49955055\n",
      "  0.50119579 1.49955055 0.50086972 0.5008932  0.50116429 1.49955055\n",
      "  0.50116429 0.50119579 0.50086972 0.50116429 0.50119579 0.50116429\n",
      "  2.49953845 1.49955055 0.50086972 1.49381794 0.50086972 0.50116429\n",
      "  1.50013908 0.50119579 2.49953845 0.50116429 3.49953591 0.50116429\n",
      "  1.50013908 0.50086972 1.49955055 3.50280156 0.50116429 0.50086972\n",
      "  0.50116429 0.50086972 0.50116429 1.49955055 0.50086972 0.50116429\n",
      "  1.50013908 0.50086972 0.50086972 0.50086972 0.50086972 1.49955055\n",
      "  0.50116429 0.50086972 0.50116429 0.50086972 0.50086972 0.50086972\n",
      "  0.50086972]]\n"
     ]
    }
   ],
   "source": [
    "# 通过fit_transform函数，我们就可以得到文档的主题模型分布在docres中。而主题词 分布则在lda.components_中。我们将其打印出来：\n",
    "print(docres)\n",
    "print('=='*30)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上述结果可以发现，文档1，文档3是属于主题1，文档2属于主题2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
