{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "数据阶段\n",
    "---\n",
    "包括数据的下载，数据的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarDataset(data.Dataset):\n",
    "    @classmethod\n",
    "    def download_or_unzip(cls, root):\n",
    "        path = os.path.join(root, cls.dirname)\n",
    "        if not os.path.isdir(path):\n",
    "            tpath = os.path.join(root, cls.filename)\n",
    "            if not os.path.isfile(tpath):\n",
    "                print('downloading')\n",
    "                urllib.request.urlretrieve(cls.url, tpath)\n",
    "            with tarfile.open(tpath, 'r') as tfile:\n",
    "                print('extracting')\n",
    "                tfile.extractall(root)\n",
    "        return os.path.join(path, '')\n",
    "\n",
    "\n",
    "class MR(TarDataset):\n",
    "\n",
    "    url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "    filename = 'rt-polaritydata.tar'\n",
    "    dirname = 'rt-polaritydata'\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "        def clean_str(string):\n",
    "            \"\"\"\n",
    "            数据预处理，使用正则化进行处理数据\n",
    "            \"\"\"\n",
    "            string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "            string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "            string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "            string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "            string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "            string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "            string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "            string = re.sub(r\",\", \" , \", string)\n",
    "            string = re.sub(r\"!\", \" ! \", string)\n",
    "            string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "            string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "            string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "            string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "            return string.strip()\n",
    "\n",
    "        text_field.preprocessing = data.Pipeline(clean_str)\n",
    "        \n",
    "        # 确定数据的格式\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "\n",
    "        if examples is None:\n",
    "            path = self.dirname if path is None else path\n",
    "            examples = []\n",
    "            with open(os.path.join(path, 'rt-polarity.neg'), errors='ignore') as f:\n",
    "                examples += [data.Example.fromlist([line, 'negative'], fields) for line in f]\n",
    "            with open(os.path.join(path, 'rt-polarity.pos'), errors='ignore') as f:\n",
    "                examples += [data.Example.fromlist([line, 'positive'], fields) for line in f]\n",
    "        super(MR, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, dev_ratio=.1, shuffle=True, root='.', **kwargs):\n",
    "        path = cls.download_or_unzip(root)\n",
    "        examples = cls(text_field, label_field, path=path, **kwargs).examples\n",
    "        \n",
    "        # 重新分布数据\n",
    "        if shuffle: \n",
    "            random.shuffle(examples)\n",
    "        dev_index = -1 * int(dev_ratio*len(examples))\n",
    "\n",
    "        return (cls(text_field, label_field, examples=examples[:dev_index]),\n",
    "                cls(text_field, label_field, examples=examples[dev_index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['-test'], dest='test', nargs=0, const=True, default=False, type=None, choices=None, help='train or test', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入配置文件\n",
    "\n",
    "parser = argparse.ArgumentParser(description='CNN text classificer')\n",
    "# learning\n",
    "parser.add_argument('-lr', type=float, default=0.001, help='initial learning rate [default: 0.001]')\n",
    "parser.add_argument('-epochs', type=int, default=256, help='number of epochs for train [default: 256]')\n",
    "parser.add_argument('-batch-size', type=int, default=64, help='batch size for training [default: 64]')\n",
    "parser.add_argument('-log-interval',  type=int, default=1,   help='how many steps to wait before logging training status [default: 1]')\n",
    "parser.add_argument('-test-interval', type=int, default=100, help='how many steps to wait before testing [default: 100]')\n",
    "parser.add_argument('-save-interval', type=int, default=500, help='how many steps to wait before saving [default:500]')\n",
    "parser.add_argument('-save-dir', type=str, default='snapshot', help='where to save the snapshot')\n",
    "parser.add_argument('-early-stop', type=int, default=1000, help='iteration numbers to stop without performance increasing')\n",
    "parser.add_argument('-save-best', type=bool, default=True, help='whether to save when get best performance')\n",
    "# data \n",
    "parser.add_argument('-shuffle', action='store_true', default=False, help='shuffle the data every epoch')\n",
    "# model\n",
    "parser.add_argument('-dropout', type=float, default=0.5, help='the probability for dropout [default: 0.5]')\n",
    "parser.add_argument('-max-norm', type=float, default=3.0, help='l2 constraint of parameters [default: 3.0]')\n",
    "parser.add_argument('-embed-dim', type=int, default=128, help='number of embedding dimension [default: 128]')\n",
    "parser.add_argument('-kernel-num', type=int, default=100, help='number of each kind of kernel')\n",
    "parser.add_argument('-kernel-sizes', type=str, default='3,4,5', help='comma-separated kernel size to use for convolution')\n",
    "parser.add_argument('-static', action='store_true', default=False, help='fix the embedding')\n",
    "# device\n",
    "parser.add_argument('-device', type=int, default=-1, help='device to use for iterate data, -1 mean cpu [default: -1]')\n",
    "# option\n",
    "parser.add_argument('-snapshot', type=str, default=None, help='filename of model snapshot [default: None]')\n",
    "parser.add_argument('-predict', type=str, default=None, help='predict the sentence given')\n",
    "parser.add_argument('-test', action='store_true', default=False, help='train or test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MR dataset\n",
    "def mr(text_field, label_field, **kargs):\n",
    "    train_data, dev_data =MR.splits(text_field, label_field)\n",
    "    text_field.build_vocab(train_data, dev_data)\n",
    "    label_field.build_vocab(train_data, dev_data)\n",
    "    train_iter, dev_iter = data.Iterator.splits(\n",
    "                                (train_data, dev_data), \n",
    "                                batch_sizes=(args.batch_size, len(dev_data)),\n",
    "                                **kargs)\n",
    "    return train_iter, dev_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在jupyter notebook 中，\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset....\")\n",
    "text_field=data.Field(lower=True)\n",
    "label_field=data.Field(sequential=False)\n",
    "\n",
    "train_iter,dev_iter=mr(text_field,label_field,device=-1,repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.LongTensor of size 47x64]\n",
      "\t[.label]:[torch.LongTensor of size 64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21108"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据\n",
    "print(list(train_iter)[0])\n",
    "# 查看词汇数量\n",
    "len(text_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "\tBATCH_SIZE=64\n",
      "\tCLASS_NUM=2\n",
      "\tDEVICE=-1\n",
      "\tDROPOUT=0.5\n",
      "\tEARLY_STOP=1000\n",
      "\tEMBED_DIM=128\n",
      "\tEMBED_NUM=21108\n",
      "\tEPOCHS=256\n",
      "\tKERNEL_NUM=100\n",
      "\tKERNEL_SIZES=[3, 4, 5]\n",
      "\tLOG_INTERVAL=1\n",
      "\tLR=0.001\n",
      "\tMAX_NORM=3.0\n",
      "\tPREDICT=None\n",
      "\tSAVE_BEST=True\n",
      "\tSAVE_DIR=snapshot\\2019-07-09_17-21-42\n",
      "\tSAVE_INTERVAL=500\n",
      "\tSHUFFLE=False\n",
      "\tSNAPSHOT=None\n",
      "\tSTATIC=False\n",
      "\tTEST=False\n",
      "\tTEST_INTERVAL=100\n"
     ]
    }
   ],
   "source": [
    "# 更新预设的超参数\n",
    "args.embed_num=len(text_field.vocab)\n",
    "args.class_num=len(label_field.vocab)-1\n",
    "\n",
    "# 设置kernel_size\n",
    "args.kernel_sizes=[int(k) for k in args.kernel_sizes.split(',')]\n",
    "args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "# 打印出模型设计的超参数\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(args.__dict__.items()):\n",
    "    print(\"\\t{}={}\".format(attr.upper(), value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型设计\n",
    "----\n",
    "关于text-cnn模型的设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型设计\n",
    "class CNN_Text(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        embed_num = args.embed_num\n",
    "        embed_dim = args.embed_dim\n",
    "        class_num = args.class_num\n",
    "        input_channels = 1\n",
    "        kernel_num = args.kernel_num\n",
    "        kernel_sizes = args.kernel_sizes\n",
    "        # 初始化embedding\n",
    "        self.embed = nn.Embedding(embed_num, embed_dim)\n",
    "        # 类似于append()\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(input_channels , kernel_num, (K, embed_dim)) for K in kernel_sizes])\n",
    "        # 设置dropout层\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        # 最后的全连接层\n",
    "        self.fc1 = nn.Linear(len(kernel_sizes)*kernel_num, class_num)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        # 激活函数\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        # 最大池化\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.args.static:\n",
    "            x = Variable(x)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logit = self.fc1(x)\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练以及初始化\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=CNN_Text(args)\n",
    "if args.snapshot is not None:\n",
    "    print(\"Loading model from {} .....\".format(args.snapshot))\n",
    "    cnn.load_state_dict(torch.load(args.snapshot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, save_dir, save_prefix, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练四步法\n",
    "def train(train_iter, dev_iter, model, args):\n",
    "    \"\"\"\n",
    "    train的四个步骤要遵守\n",
    "    \"\"\"\n",
    "    # 确定优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        for batch in train_iter:\n",
    "            feature, target = batch.text, batch.label\n",
    "            feature=feature.data.t()\n",
    "            target=target.data.sub(1)  # batch first, index align\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(feature)\n",
    "            # 确定损失函数为交叉熵损失函数\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            steps += 1\n",
    "            # 输出日志\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "                accuracy = 100.0 * corrects/batch.batch_size\n",
    "                sys.stdout.write(\n",
    "                    '\\rBatch[{}] - loss: {}  acc: {}%({}/{})'.format(steps, \n",
    "                                                                             loss.item,\n",
    "                                                                             accuracy,\n",
    "                                                                             corrects,\n",
    "                                                                             batch.batch_size))\n",
    "            if steps % args.test_interval == 0:\n",
    "                dev_acc = eval(dev_iter, model, args)\n",
    "                if dev_acc > best_acc:\n",
    "                    best_acc = dev_acc\n",
    "                    last_step = steps\n",
    "                    if args.save_best:\n",
    "                        save(model, args.save_dir, 'best', steps)\n",
    "                else:\n",
    "                    if steps - last_step >= args.early_stop:\n",
    "                        print('early stop by {} steps.'.format(args.early_stop))\n",
    "            elif steps % args.save_interval == 0:\n",
    "                save(model, args.save_dir, 'snapshot', steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_iter, model, args):\n",
    "    \"\"\"\n",
    "    验证\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in data_iter:\n",
    "        feature, target = batch.text, batch.label\n",
    "        feature=feature.data.t()\n",
    "        target=target.data.sub(1)  # batch first, index align\n",
    "\n",
    "        logit = model(feature)\n",
    "        loss = F.cross_entropy(logit, target, size_average=False)\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logit, 1)\n",
    "                     [1].view(target.size()).data == target.data).sum()\n",
    "\n",
    "    size = len(data_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    print('\\nEvaluation - loss: {}  acc: {}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def predict(text, model, text_field, label_feild, cuda_flag):\n",
    "    \"\"\"\n",
    "    预测\n",
    "    \"\"\"\n",
    "    assert isinstance(text, str)\n",
    "    model.eval()\n",
    "    # text = text_field.tokenize(text)\n",
    "    text = text_field.preprocess(text)\n",
    "    text = [[text_field.vocab.stoi[x] for x in text]]\n",
    "    x = torch.tensor(text)\n",
    "    x = autograd.Variable(x)\n",
    "    if cuda_flag:\n",
    "        x = x.cuda()\n",
    "    print(x)\n",
    "    output = model(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    #return label_feild.vocab.itos[predicted.data[0][0]+1]\n",
    "    return label_feild.vocab.itos[predicted.data[0]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[100] - loss: <built-in method item of Tensor object at 0x0000013E92FE6948>  acc: 64%(41/64)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\python\\env\\nlp\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.599169  acc: 67.0000%(720/1066) \n",
      "\n",
      "Batch[200] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 95%(61/64))\n",
      "Evaluation - loss: 0.591676  acc: 70.0000%(751/1066) \n",
      "\n",
      "Batch[300] - loss: <built-in method item of Tensor object at 0x0000013E92FF4048>  acc: 93%(56/60)\n",
      "Evaluation - loss: 0.574262  acc: 70.0000%(751/1066) \n",
      "\n",
      "Batch[400] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.608738  acc: 71.0000%(766/1066) \n",
      "\n",
      "Batch[500] - loss: <built-in method item of Tensor object at 0x0000013E92FE6900>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.635403  acc: 72.0000%(776/1066) \n",
      "\n",
      "Batch[600] - loss: <built-in method item of Tensor object at 0x0000013E92FF4828>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.665352  acc: 71.0000%(762/1066) \n",
      "\n",
      "Batch[700] - loss: <built-in method item of Tensor object at 0x0000013E92FF4168>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.699952  acc: 72.0000%(773/1066) \n",
      "\n",
      "Batch[800] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AF8>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.710924  acc: 72.0000%(772/1066) \n",
      "\n",
      "Batch[900] - loss: <built-in method item of Tensor object at 0x0000013E92FE6B40>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.733147  acc: 72.0000%(768/1066) \n",
      "\n",
      "Batch[1000] - loss: <built-in method item of Tensor object at 0x0000013E92FF4828>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.757543  acc: 72.0000%(773/1066) \n",
      "\n",
      "Batch[1100] - loss: <built-in method item of Tensor object at 0x0000013E92FF43F0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.770715  acc: 72.0000%(771/1066) \n",
      "\n",
      "Batch[1200] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.780269  acc: 72.0000%(769/1066) \n",
      "\n",
      "Batch[1300] - loss: <built-in method item of Tensor object at 0x0000013E92FE6CF0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.794285  acc: 71.0000%(767/1066) \n",
      "\n",
      "Batch[1400] - loss: <built-in method item of Tensor object at 0x0000013E92FE6CF0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.804125  acc: 72.0000%(772/1066) \n",
      "\n",
      "Batch[1500] - loss: <built-in method item of Tensor object at 0x0000013E92FE6900>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.822254  acc: 72.0000%(774/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1600] - loss: <built-in method item of Tensor object at 0x0000013E92FF43F0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.825995  acc: 72.0000%(770/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1700] - loss: <built-in method item of Tensor object at 0x0000013E92FE9E10>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.835634  acc: 72.0000%(768/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1800] - loss: <built-in method item of Tensor object at 0x0000013E92FF45A0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.846330  acc: 72.0000%(775/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1900] - loss: <built-in method item of Tensor object at 0x0000013E92FE6A20>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.861067  acc: 72.0000%(774/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2000] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.867521  acc: 72.0000%(771/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2100] - loss: <built-in method item of Tensor object at 0x0000013E92FE6EA0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.875678  acc: 72.0000%(772/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2200] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.884472  acc: 72.0000%(768/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2300] - loss: <built-in method item of Tensor object at 0x0000013E92FEFAF8>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.896516  acc: 72.0000%(778/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2400] - loss: <built-in method item of Tensor object at 0x0000013E92FEFAF8>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.900782  acc: 72.0000%(775/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2500] - loss: <built-in method item of Tensor object at 0x0000013E92FE6900>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.905274  acc: 72.0000%(776/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2600] - loss: <built-in method item of Tensor object at 0x0000013E92FE6B40>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.917593  acc: 72.0000%(772/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2700] - loss: <built-in method item of Tensor object at 0x0000013E92FE6EA0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.921380  acc: 72.0000%(775/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2800] - loss: <built-in method item of Tensor object at 0x0000013E92FE6EA0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.930276  acc: 72.0000%(773/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[2900] - loss: <built-in method item of Tensor object at 0x0000013E92FF45A0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.934098  acc: 72.0000%(772/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3000] - loss: <built-in method item of Tensor object at 0x0000013E92FF45A0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.945874  acc: 72.0000%(775/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3100] - loss: <built-in method item of Tensor object at 0x0000013E92FE6A20>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.949678  acc: 72.0000%(773/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3200] - loss: <built-in method item of Tensor object at 0x0000013E92FEFAF8>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.957982  acc: 72.0000%(774/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3300] - loss: <built-in method item of Tensor object at 0x0000013E92FE6CF0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.963715  acc: 72.0000%(776/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3400] - loss: <built-in method item of Tensor object at 0x0000013E92FE6CF0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.968645  acc: 72.0000%(774/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3500] - loss: <built-in method item of Tensor object at 0x0000013E92FEFAF8>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.974403  acc: 72.0000%(774/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3600] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 0.982836  acc: 72.0000%(775/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3700] - loss: <built-in method item of Tensor object at 0x0000013E92FE6B40>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.989655  acc: 72.0000%(777/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3800] - loss: <built-in method item of Tensor object at 0x0000013E92FE6B40>  acc: 100%(64/64)\n",
      "Evaluation - loss: 0.994492  acc: 72.0000%(775/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[3900] - loss: <built-in method item of Tensor object at 0x0000013E92FE6900>  acc: 100%(60/60)\n",
      "Evaluation - loss: 1.001163  acc: 72.0000%(776/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4000] - loss: <built-in method item of Tensor object at 0x0000013E92FF45A0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.006927  acc: 72.0000%(777/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4100] - loss: <built-in method item of Tensor object at 0x0000013E92FE9E10>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.011039  acc: 72.0000%(776/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4200] - loss: <built-in method item of Tensor object at 0x0000013E92FE6CF0>  acc: 100%(60/60)\n",
      "Evaluation - loss: 1.019419  acc: 72.0000%(777/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4300] - loss: <built-in method item of Tensor object at 0x0000013E92FE6B40>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.027525  acc: 72.0000%(773/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4400] - loss: <built-in method item of Tensor object at 0x0000013E92FF43F0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.031302  acc: 72.0000%(778/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4500] - loss: <built-in method item of Tensor object at 0x0000013E92FF4168>  acc: 100%(60/60)\n",
      "Evaluation - loss: 1.034909  acc: 72.0000%(777/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4600] - loss: <built-in method item of Tensor object at 0x0000013E92FF43A8>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.041123  acc: 72.0000%(776/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4700] - loss: <built-in method item of Tensor object at 0x0000013E92FF4168>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.048429  acc: 72.0000%(778/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[4800] - loss: <built-in method item of Tensor object at 0x0000013E92FF41F8>  acc: 100%(60/60)\n",
      "Evaluation - loss: 1.051793  acc: 72.0000%(776/1066) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stop by 1000 steps.\n",
      "Batch[4900] - loss: <built-in method item of Tensor object at 0x0000013E92FE6CF0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.059559  acc: 72.0000%(777/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[5000] - loss: <built-in method item of Tensor object at 0x0000013E92FE9AB0>  acc: 100%(64/64)\n",
      "Evaluation - loss: 1.061943  acc: 72.0000%(778/1066) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[5062] - loss: <built-in method item of Tensor object at 0x0000013E92FE9E10>  acc: 100%(64/64)"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train(train_iter,dev_iter,cnn,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
